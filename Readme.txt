This project is a voice-enabled AI mock interview assistant that:
Accepts a userâ€™s resume
Conducts an adaptive interview using voice-based interaction
Evaluates answers
Generates a performance report and a personalized learning roadmap
Stores all data in Supabase
The backend is powered by FastAPI, with speech-to-text, text-to-speech, and adaptive question generation.


aimockinterview/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __pycache__/                # Python cache files (auto-generated)
â”‚   â”œâ”€â”€ credentials/                # (Optional) stores API/service credentials if needed
â”‚   â”‚
â”‚   â”œâ”€â”€ ml/                         # ğŸ§  Core backend + AI logic lives here
â”‚   â”‚   â”œâ”€â”€ __pycache__/            # Cache for compiled Python files
â”‚   â”‚   â”œâ”€â”€
â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ api.py                  # ğŸš€ Main FastAPI app (core routes & API endpoints)
â”‚   â”‚   â”œâ”€â”€ main.py                 # CLI-based version of the interview for local testing
â”‚   â”‚   â”œâ”€â”€ config.py               # Handles environment variables and app configuration
â”‚   â”‚   â”œâ”€â”€ resume_parser.py        # Extracts structured info (role, skills, exp) from uploaded resume
â”‚   â”‚   â”œâ”€â”€ supabase_config.py      # Supabase setup, storage, and DB helper functions
â”‚   â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies list
â”‚   â”‚   â”œâ”€â”€ .env                    # Environment variables (API keys, Supabase URL, etc.)
â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Evaluation.py           # Evaluates user's answers & generates scores
â”‚   â”‚   â”œâ”€â”€ question_generator.py   # Dynamically generates next question based on answer context
â”‚   â”‚   â”œâ”€â”€ report_generator.py     # Compiles interview performance report
â”‚   â”‚   â”œâ”€â”€ roadmap.py              # Builds personalized learning roadmap
â”‚   â”‚   â”œâ”€â”€ speech_to_text.py       # Converts user's speech (audio) to text using Google STT
â”‚   â”‚   â”œâ”€â”€ text_to_speech.py       # Converts interviewerâ€™s text to audio (ElevenLabs + pyttsx3 fallback)
â”‚   â”‚
â”‚   â””â”€â”€ parsed_resume.json          # Stores temporarily parsed resume data
â”‚
â”œâ”€â”€ venv/                           # Virtual environment (ignored in Git)
â””â”€â”€ README.md                       # Project documentation (this file)


# Installation Process

# Clone the Repo
git clone https://github.com/your-repo-name.git
cd aimockinterview

#Create A virtual environment
python -m venv venv
venv\Scripts\activate     # On Windows
source venv/bin/activate  # On Mac/Linux

# Install the Dependencies
pip install -r requirements.txt

# Run the FastApi server using
uvicorn backend.api:app --reload




# API endpoints Overview
1)POST /api/resume/upload
Uploads and parses a resume into structured JSON data.

2ï¸âƒ£ Get Available Voices
GET /api/interview/voices
Lists available interviewer avatars (for frontend voice selection).

3ï¸âƒ£ Stepwise Interview (Real-Time Q&A
POST /api/interview/answer
Also generated questions from text to speech with the help of Gemini and TTS models and elevnlabs voices
Handles user answers â€” either text or audio.

Handles user answers â€” either text or audio.
| Key                | Type | Description                                        |
| ------------------ | ---- | -------------------------------------------------- |
| `session_id`       | text | Optional session ID (auto-created if not provided) |
| `user_name`        | text | Userâ€™s name                                        |
| `difficulty`       | text | Difficulty level (â€œeasyâ€, â€œmediumâ€, â€œhardâ€)        |
| `voice_name`       | text | Interviewer voice name (â€œMonikaâ€, etc.)            |
| `resume_data`      | text | Parsed resume JSON                                 |
| `current_question` | text | Current question being answered                    |
| `user_answer`      | text | Text answer (optional if uploading audio)          |
| `audio_file`       | file | Voice answer in `.wav` or `.mp3` format            |


4ï¸âƒ£ Stop Interview & Generate Report

POST /api/interview/stop
Generates:
Evaluation Summary
Feedback Report
Personalized Learning Roadmap
And stores everything in Supabase.


Backend Logic Flow

| Step | File                        | Description                                                      |
| ---- | --------------------------- | ---------------------------------------------------------------- |
| 1    | **`resume_parser.py`**      | Extracts structured data from the uploaded resume                |
| 2    | **`question_generator.py`** | AI generates first interview question                            |
| 3    | **`text_to_speech.py`**     | Converts each question to audio (ElevenLabs / pyttsx3 fallback)  |
| 4    | **`speech_to_text.py`**     | Converts userâ€™s audio answers to text (Google STT)               |
| 5    | **`Evaluation.py`**         | Analyzes answer quality                                          |
| 6    | **`report_generator.py`**   | Generates final performance summary                              |
| 7    | **`roadmap.py`**            | Builds AI-driven learning plan                                   |
| 8    | **`supabase_config.py`**    | Saves everything to Supabase (sessions, reports, audio, roadmap) |
| 9    | **`api.py`**                | Coordinates all steps & defines REST endpoints                   |




#How api.py Works (Summary)

User uploads a resume â†’ parsed JSON saved to Supabase.
System begins interview (/api/interview/answer):
Sends first question.
Converts question text â†’ audio via ElevenLabs.
User responds:
Either via text input (user_answer)
Or via voice recording (audio_file)
The backend converts the userâ€™s speech â†’ text â†’ evaluates â†’ generates the next question dynamically.
After a few rounds, user says stop â†’ system:
Saves conversation logs
Generates evaluation, report, and roadmap
Stores everything to Supabase.



ğŸ§‘â€ğŸ’» Frontend (Cleonâ€™s Responsibilities)

Hereâ€™s what Cleon needs to implement on the frontend ğŸ‘‡
ğŸ¤ 1. Enable Voice Recording

Use the Web MediaRecorder API to capture the userâ€™s voice.




ğŸ”Š 2. Play the Interviewerâ€™s Voice

The backend returns both:
audio_base64 (base64 encoded string)
audio_url (Supabase public URL)
Cleon should prefer using the audio_url for smoother playback:

const audio = new Audio(data.audio_url);
audio.play();

If the URL is null, fallback to base64:
const audio = new Audio(`data:audio/mp3;base64,${data.audio_base64}`);
audio.play();


ğŸ“Š 4. Show Final Evaluation and Roadmap

After calling /api/interview/stop, Cleon should:
Display the evaluation summary
Render the roadmap as an interactive checklist
Provide download options for the report




Outputs Stored in Supabase 
| Table                | Content                              |
| -------------------- | ------------------------------------ |
| `interview_sessions` | All Q&A logs                         |
| `evaluations`        | AI evaluation scores                 |
| `reports`            | Summary report JSON                  |
| `roadmaps`           | Personalized roadmap JSON            |
| `audio` (bucket)     | Interviewerâ€™s and userâ€™s audio files |
